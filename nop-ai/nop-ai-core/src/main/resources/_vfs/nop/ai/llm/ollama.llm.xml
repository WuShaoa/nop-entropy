<llm x:schema="/nop/schema/ai/llm.xdef" xmlns:x="/nop/schema/xdsl.xdef"
     apiStyle="ollama">

    <!--    <baseUrl>http://localhost:11434/</baseUrl>-->
    <chatUrl>/api/chat</chatUrl>
    <generateUrl>/api/generate</generateUrl>
    <embedUrl>/api/embed</embedUrl>

    <request seedPath="options.seed"
             topKPath="options.top_k"
             topPPath="options.top_p"
             temperaturePath="options.temperature"
             stopPath="options.stop"
             maxTokensPath="options.num_predict"
             contextLengthPath="options.num_ctx"
             perSeqContextLengthPath="options.n_ctx_per_seq"
    />

    <response contentPath="message.content"
              rolePath="message.role"
              promptTokensPath="prompt_eval_count"
              completionTokensPath="eval_count"
              statusPath="done"
              errorPath="error"
    />

</llm>